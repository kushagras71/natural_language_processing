# Natural Language Processing

Natural Language Processing


TensorFlow Embeddings:

1. 20 dimensional embedding vectors: https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1

2. 50 dimensional embedding vectors: https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1

3. 128 dimensional embedding vectors: https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1


<hr>


Some Reference Material - 

<ol>
<li>  Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019) - 
  
  https://arxiv.org/abs/1910.10683 </li>
  
 <li>  Reformer: The Efficient Transformer (Kitaev et al, 2020) - 
  
  https://arxiv.org/abs/2001.04451
</li>
<li>
   Attention Is All You Need (Vaswani et al, 2017) - 
  
  https://arxiv.org/abs/1706.03762
  
  <li>
   Deep contextualized word representations (Peters et al, 2018)-
  
  https://arxiv.org/pdf/1802.05365.pdf
</li>
<li> The Illustrated Transformer (Alammar, 2018)- 
  
  http://jalammar.github.io/illustrated-transformer/
</li> 
<li> The Illustrated GPT-2 (Visualizing Transformer Language Models) (Alammar, 2019)-
  
  http://jalammar.github.io/illustrated-gpt2/
  </li>
<li>  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)-
  
  https://arxiv.org/abs/1810.04805
  </li>
  <li>  How GPT3 Works - Visualizations and Animations (Alammar, 2020)-
  
  http://jalammar.github.io/how-gpt3-works-visualizations-animations/
  </li>
</ol>
<hr>

Transformer Beyond NLP -

<ul>
 <li>Jukebox - A neural network that generates music!-
   
   https://openai.com/blog/jukebox/</li>
   
   <li> GPT-3 Can also help with auto-programming!-
  
  https://beta.openai.com/?app=productivity&example=4_2_0</li>
 
 </ul>
